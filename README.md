# ğŸ‡»ğŸ‡³ Vietnamese Vision-Language MoE

<p align="center">
  <img src="https://img.shields.io/badge/Python-3.11+-blue.svg" alt="Python">
  <img src="https://img.shields.io/badge/PyTorch-2.0+-red.svg" alt="PyTorch">
  <img src="https://img.shields.io/badge/Transformers-4.40+-yellow.svg" alt="Transformers">
  <img src="https://img.shields.io/badge/License-MIT-green.svg" alt="License">
</p>

A **Vietnamese Vision-Language model** using Mixture of Experts (MoE) architecture, featuring:

- ğŸ¯ **DeepSeek-style MoE**: Shared + Routed Experts with Top-k routing
- ğŸ”„ **Full Encoder Reuse**: Load trained text_encoder & projections from checkpoint
- ğŸ–¼ï¸ **Token Compression**: Resampler to balance visual (50) vs text (32) tokens
- âš¡ **Sparse Upcycling**: Initialize MoE experts from trained projection weights
- ğŸ‡»ğŸ‡³ **Vietnamese Support**: XLM-RoBERTa for Vietnamese text

## ğŸš€ Quick Start

```bash
# Install dependencies
conda create -n moe python=3.11
conda activate moe
pip install torch torchvision transformers datasets tqdm

# Train Stage 1: Text Alignment (Teacher Learning)
python train_contrastive.py --training_stage teacher --num_epochs 15

# Train Stage 2: Vision-Language (Contrastive Learning)
python train_contrastive.py --training_stage contrastive --stage1_checkpoint checkpoint.pt

# Stage 3: Upcycle to MoE (loads ALL trained weights + init MoE from projections)
python upcycle_to_moe.py \
    --dense_checkpoint checkpoint.pt \
    --output_dir outputs/vlmoe \
    --num_routed_experts 64 \
    --num_shared_experts 2

# Stage 4: Train VL-MoE (contrastive + MoE losses)
python train_vlmoe.py \
    --upcycled_checkpoint outputs/vlmoe/vlmoe_upcycled.pt \
    --num_epochs 10 \
    --batch_size 512 \
    --learning_rate 2e-5
```

## ğŸ“ Project Structure

```
moe/
â”œâ”€â”€ model/
â”‚   â”œâ”€â”€ vlmoe/           # VL-MoE model
â”‚   â”‚   â”œâ”€â”€ vlmoe_model.py   # Main model (extends mm_encoder)
â”‚   â”‚   â””â”€â”€ upcycling.py     # Sparse upcycling utilities
â”‚   â”œâ”€â”€ moe/             # MoE layers
â”‚   â”‚   â””â”€â”€ deepseek_moe.py  # DeepSeek-style MoE
â”‚   â”œâ”€â”€ projector/       # Visual token compression
â”‚   â”‚   â””â”€â”€ visual_projector.py  # Resampler
â”‚   â”œâ”€â”€ mm/              # Dense multimodal encoder
â”‚   â”‚   â””â”€â”€ mm_encoder.py    # Original trained model
â”‚   â”œâ”€â”€ text/            # Text encoder
â”‚   â”‚   â””â”€â”€ text_encoder.py  # XLM-RoBERTa
â”‚   â””â”€â”€ vision/          # Vision encoder
â”œâ”€â”€ losses/              # Loss functions
â”œâ”€â”€ data/                # Data loaders
â”œâ”€â”€ train_contrastive.py # Training script (Stage 1 & 2)
â”œâ”€â”€ upcycle_to_moe.py    # Upcycling script (Stage 3)
â”œâ”€â”€ train_vlmoe.py       # VL-MoE training script (Stage 4)
â””â”€â”€ README_VLMOE.md      # Detailed documentation
```

## ğŸ“– Documentation

See [README_VLMOE.md](README_VLMOE.md) for detailed documentation including:

- Architecture diagrams
- Training pipeline
- Configuration options
- Tensor shape flow
- API reference
- Sparse upcycling details

## ğŸ—ï¸ Architecture Overview

```
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘  FROM CHECKPOINT (trained):                                      â•‘
â•‘  â”œâ”€â”€ text_encoder (XLM-RoBERTa)      âœ“ Full encoder             â•‘
â•‘  â”œâ”€â”€ vision_text_model (CLIP)        âœ“ Frozen backbone          â•‘
â•‘  â”œâ”€â”€ xlmr_text_projection (768â†’512)  âœ“ Trained                  â•‘
â•‘  â”œâ”€â”€ text_projection_output (512â†’768) âœ“ Trained                 â•‘
â•‘  â”œâ”€â”€ vision_projection_output        âœ“ Trained                  â•‘
â•‘  â””â”€â”€ logit_scale, logit_bias         âœ“ Trained                  â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘  NEW FOR MOE:                                                    â•‘
â•‘  â”œâ”€â”€ visual_projector (Resampler)    - Token compression        â•‘
â•‘  â”œâ”€â”€ modality_embedding              - Visual/text distinction  â•‘
â•‘  â”œâ”€â”€ moe_layers (Ã—N)                 - DeepSeek MoE             â•‘
â•‘  â”‚   â”œâ”€â”€ shared_experts (Ã—2)         - From projections         â•‘
â•‘  â”‚   â””â”€â”€ routed_experts (Ã—64)        - From projections + noise â•‘
â•‘  â””â”€â”€ output heads                    - Task-specific            â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

**Data Flow:**
```
Image [B,3,224,224] â†’ CLIP â†’ [B,50,768] â†’ Resampler â†’ [B,64,768] â”€â”
                                                                  â”œâ†’ MoE Layers â†’ Output
Text  [B,32]        â†’ XLM-RoBERTa       â†’ [B,32,768] â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**MoE Layer Structure:**
- **Shared Experts (Ã—2)**: Always active, capture common patterns
- **Routed Experts (Ã—64)**: Top-2 selected per token, specialized

## ğŸ”„ Sparse Upcycling

Upcycling loads **ALL** trained weights and uses projections to initialize MoE:

| Component | From Checkpoint | Notes |
|-----------|-----------------|-------|
| `text_encoder` | âœ… Full | All XLM-RoBERTa layers |
| `vision_text_model` | âœ… Full | CLIP (frozen) |
| `xlmr_text_projection` | âœ… | 768 â†’ 512 |
| `text_projection_output` | âœ… | 512 â†’ 768 |
| `vision_projection_output` | âœ… | 768 â†’ 768 |
| `logit_scale/bias` | âœ… | Temperature |
| **MoE shared experts** | âœ… via upcycling | Exact copy |
| **MoE routed experts** | âœ… via upcycling | Copy + noise |
| `visual_projector` | âŒ Random | Need training |
| `modality_embedding` | âŒ Random | Need training |

## ğŸ“Š Training Pipeline

| Stage | Script | Epochs | Description |
|-------|--------|--------|-------------|
| 1. Teacher | `train_contrastive.py` | 15 | XLM-R â†” CLIP text alignment |
| 2. Contrastive | `train_contrastive.py` | 10 | Vision-language alignment |
| 3. Upcycle | `upcycle_to_moe.py` | - | Convert to MoE |
| 4. Train MoE | `train_vlmoe.py` | 5-10 | Fine-tune MoE |

### Recommended MoE Settings

| Parameter | Value | Notes |
|-----------|-------|-------|
| `num_routed_experts` | 64 | Specialized experts |
| `num_shared_experts` | 2 | Always active |
| `num_experts_per_tok` | 2 | Top-k routing |
| `learning_rate` | 2e-5 | Lower than dense |
| `batch_size` | 512 | + gradient accumulation |

## ğŸ”— References

- [DeepSeek-MoE](https://arxiv.org/abs/2401.06066)
- [LIMoE](https://arxiv.org/abs/2206.02770)
- [Sparse Upcycling](https://arxiv.org/abs/2212.05055)
- [CLIP](https://arxiv.org/abs/2103.00020)
- [XLM-RoBERTa](https://arxiv.org/abs/1911.02116)

## ğŸ“„ License

MIT License
